## Notes going into

### Jacob

- Interaction of Annotator data quality with task / domain type?
	- needs to address difference of domains / annotations tasks
- expert = gold standard
	- uncertainty of annotations?
- Could reference uncertainty of labels in intro as a (potential) driver for disagreement?
- what is an annotator?
	- human or human / llm?
	- maybe clearer usage?
- re Annotator Types: There are (probably) also tasks that are easy enough that laypeople are as good as experts at them?
- Maybe talk about models learning from their own output in the long run? "dogfooding"
- table of annotators?!
- comparison with other models will be gold!
- What's the takeaway / tips and suggestions for future research?

- further research
	- multiple runs of labeling w/ same model
	- active learning
	- other providers
- !! price

### Olga

- rethink framing of article
	- AI surveys as a potential tool to help solve ... what?
	- not only response burden also quality (maybe much better sell)
- what tool was she using for generating the MS?
- very long intro?
	- I think you can bring the intro "closer" to the actual study / what was done?

- uniquely positioned: follow up questions?
- one or two simple figure(s) to illustrate some results?
