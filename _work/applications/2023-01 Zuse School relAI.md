---
deadline: 09-01-2023
url: https://zuseschoolrelai.de/application/
---
[Application portal](https://www.portal.graduatecenter.uni-muenchen.de/ocgc/relAI)

### Documents
-   Your CV
-   Copies of your relevant diplomas and transcripts*
-   Proof of English language proficiency (for PhD applicants only, level C1 or equivalent, or University degree that was taught mainly in English)**
-   Contact details of the two people who you picked to provide reference letters for you
	- Frauke
	- Sam
-   *Letter of motivation*
-   *Short text about your research interests*
-   Names of relAI fellows you would like to work with (see [Fellows](https://zuseschoolrelai.de/fellows-overview/))

> Wichtig ist dass Du bei deiner Bewerbung schreiben kannst wie deine Forschungspläne und Interessen die Aspekte von relAI abdecken.


## Letter of motivation
> Please let us know what motivates you to apply for a position with relAI, and why the topic of relAI matters to you.

With this letter of motivation, I would like to express my interest in a position as an (associated) PhD student at the Konrad Zuse School of Excellence in Reliable AI. I learned of this position and the new graduate school, after joining Prof. Kreuter's chair at the Ludwig-Maximilians-Universität (LMU) in July 2022. Upon hearing of the opportunity, I was intrigued, since I believe that the topics falling under the umbrella term of reliable AI are severely understudied and deserve immediate attention.

Since the advent of ChatGPT in late 2022, even the last skeptics should be convinced by the potential of AI systems. And while the capabilities of ChatGPT are certainly impressive, it also demonstrated the lingering issues with AI on a grand scale; despite safeguards in place, people managed to trick the model into producing unanticipated harm- and hateful output within mere hours of access.

This impressively demonstrates how much work is still to do in understanding how we can develop AI systems with understandable and reliable outcomes. I believe that these reliability issues are especially prominent when it comes to AI systems being used for automated decision-making (ADM), given the mix of high-stakes decisions and difficult-to-understand models. Even when assuming good intentions, it's easy to cause harm with naive ADM systems making unfair decisions. However, when personal or institutional incentives are factored in, it makes for an even more dangerous combination.

To combat this, I am planning to develop new tools and analyses to better understand how ADM systems are affected by design decisions, increasing both their safety and security. I also hope to improve the understanding of factors that are necessary for people to trust AI systems across different contexts, to help inform the development of more responsible AI systems in the future. This information could be used to craft guidelines for responsible ADM systems that will be more aligned with the values of the people impacted by them.

The Konrad Zuse School of Excellence in Reliable AI would provide the perfect environment to further these goals where I could join a network of like-minded people to address these issues and many more. I believe that I would be an excellent fit for the school as I uniquely combine practical experience from both research and industry with a strong interdisciplinary background and years of experience.

## Short text about your research interests
> Please sketch out briefly what topic(s) you would be interested to work on.

I am interested in developing new techniques to improve the understanding of AI systems in general and automated decision-making systems in particular. I believe that one opportunity to do this that has been overlooked until now, is by building on techniques from the field of Psychology. Since the field is currently developing various methods to cope with the reproducibility crisis, I think that some of these learnings can be transferred to produce more reliable AI. As an example, one project I'm interested in working on is to introduce multiverse analyses to AI research. In a multiverse analysis one turns implicit choices during the model design or analysis into explicit ones and explores how they affect the final outcome of an analysis – or AI model. I believe that this technique can bring issues of safety to light and produce more resilient AI models, with the scope of their decision space clearly outlined. This is especially relevant when it comes to algorithmic decision making where besides issues of performance or accuracy, also issues of fairness have to be taken into account.

Another research area I find highly interesting is understanding the perspective of people affected by AI. Existing research has demonstrated that the developers of AI systems have different views and attitudes from the people who are affected by thos AI systems. If we want to be able to create responsible AI which takes the affected people's needs into account, we will first have to understand these needs. One approach of doing this could be to use citizen science to gather large scale data on the necessary factors for people to trust AI systems. This way, one might also be able to understand people's perspectives on AI from populations beyond the so-called WEIRD (western, educated, industrialized, rich, democratic) convenience samples used in AI fairness research so far. This data could be used to inform future guidelines on the design of responsible AI.


## Previous research projects
>If you have previously worked on research projects, please tell us about them.

One of the largest research projects I've been involved in so far was the creation of the citizen science platform themusiclab.org, where we create gamified web experiments that people can freely participate in. The platform has been a great success with more than 1 million people from across the world participating per year. The data from this platform lead to several published papers in major peer-reviewed journals.

Further, I’m the main author of a new R package where we use machine learning to help with the accurate coding of occupations. The package provides an end-to-end solution from data collection with automatically generated answer options, all the way to standardized occupation codes. The coding of occupations is a particularly hard classification problem due to the large number and high ambiguity of categories, with even human experts struggling to achieve adequate reliability. The problem is further aggravated by difficulties around obtaining and sharing training data due to the relevant free text responses being highly personal and difficult to anonymize.

For my Master's thesis, I used 1.5 million ratings of preference for highly unfamiliar music by more than 200 hundred thousand people from the citizen-science platform and analyzed them for patterns in a largely exploratory analysis. I discovered surprisingly little variation between countries or languages in preference, when compared to variation between songs or individuals. Using various machine learning models, I tried to predict individual as well as aggregate musical preference. This highlighted the stark contrast with multiple models performing well in predicting aggregate musical preference, but individual musical preference being difficult to predict accurately.