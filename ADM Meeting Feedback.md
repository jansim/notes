- which one's lead to binary split?
- binarization as major problem
- make assumptions explicit; hihglight implicity of existing processing / filtering
- "cluture of benchmarking" -> no ordering on multidimensional things?!
- bigger overall question?
- beyond fairML?
- refer back to, explain (?), critical data studies
- same underlying problem leading to biased data and these evaluation issues?
- expand on recommendations a bit

- [ ] Sensitive -> Protected in Sankey
